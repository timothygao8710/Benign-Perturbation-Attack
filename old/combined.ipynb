{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = '\\n\\nConsider the following MCQ Questions. response with the letter:\\n\\n'\n",
    "\n",
    "def getOptionsString(options):\n",
    "    res = ''\n",
    "    for i in options:\n",
    "        res += i + ': ' + options[i]\n",
    "        res += '\\n'\n",
    "    return res\n",
    "\n",
    "def getLetter(llm_response):\n",
    "    for i in llm_response.split('\\n'):\n",
    "        if(len(i) > 1 and i[0] != '#'):\n",
    "            return i[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>options</th>\n",
       "      <th>meta_info</th>\n",
       "      <th>model_letter</th>\n",
       "      <th>model_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A 4670-g (10-lb 5-oz) male newborn is delivere...</td>\n",
       "      <td>D</td>\n",
       "      <td>{'A': 'Nerve conduction study', 'B': 'Surgical...</td>\n",
       "      <td>step2</td>\n",
       "      <td>E</td>\n",
       "      <td>E Splinting of the arm. The explanation is as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A 66-year-old man comes to the physician for a...</td>\n",
       "      <td>F</td>\n",
       "      <td>{'A': 'Cirrhosis', 'B': 'Acute lymphoblastic l...</td>\n",
       "      <td>step2</td>\n",
       "      <td>F</td>\n",
       "      <td>F Acute myelogenous leukemia. The patient's s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A 40-year-old man comes to the physician becau...</td>\n",
       "      <td>E</td>\n",
       "      <td>{'A': 'Asphyxia', 'B': 'Achlorhydria', 'C': 'T...</td>\n",
       "      <td>step2</td>\n",
       "      <td>D</td>\n",
       "      <td>D Megaloblastic anemia. \\n\\nExplanation: \\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A 4-year-old boy is brought to the emergency d...</td>\n",
       "      <td>E</td>\n",
       "      <td>{'A': \"Get consent from the patient's brother\"...</td>\n",
       "      <td>step2</td>\n",
       "      <td>E</td>\n",
       "      <td>E. Perform emergency laparotomy.\\n\\nExplanati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A 9-year-old girl is brought to the emergency ...</td>\n",
       "      <td>C</td>\n",
       "      <td>{'A': 'Oculomotor nerve damage', 'B': 'Retrobu...</td>\n",
       "      <td>step2</td>\n",
       "      <td>F</td>\n",
       "      <td>F Abducens nerve damage. The patient's sympto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14364</th>\n",
       "      <td>A 9-year-old boy is brought to the clinic by h...</td>\n",
       "      <td>A</td>\n",
       "      <td>{'A': 'Defect of the septum secundum', 'B': 'F...</td>\n",
       "      <td>step1</td>\n",
       "      <td>B</td>\n",
       "      <td>B Failure of the septum primum to fuse with t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14365</th>\n",
       "      <td>A 3-month-old infant is brought to the E.R. by...</td>\n",
       "      <td>B</td>\n",
       "      <td>{'A': 'Recent pharyngitis', 'B': 'Recent consu...</td>\n",
       "      <td>step1</td>\n",
       "      <td>B</td>\n",
       "      <td>B Recent consumption of honey. The reason for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14366</th>\n",
       "      <td>A 36-year-old woman with no significant medica...</td>\n",
       "      <td>A</td>\n",
       "      <td>{'A': 'Formation of free radicals', 'B': 'Inhi...</td>\n",
       "      <td>step1</td>\n",
       "      <td>D</td>\n",
       "      <td>D Binding to the 50S subunit of the ribosome....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14367</th>\n",
       "      <td>A 68-year-old male is brought to his primary c...</td>\n",
       "      <td>A</td>\n",
       "      <td>{'A': 'Accumulations of beta-pleated sheets', ...</td>\n",
       "      <td>step1</td>\n",
       "      <td>D</td>\n",
       "      <td>D Intracellular rounded hyperphosphorylated t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14368</th>\n",
       "      <td>A 7-year-old girl presents for a follow-up vis...</td>\n",
       "      <td>D</td>\n",
       "      <td>{'A': 'Substantia nigra', 'B': 'Hippocampus', ...</td>\n",
       "      <td>step1</td>\n",
       "      <td>D</td>\n",
       "      <td>D Amygdala. The reason for this is that the p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14369 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question answer  \\\n",
       "0      A 4670-g (10-lb 5-oz) male newborn is delivere...      D   \n",
       "1      A 66-year-old man comes to the physician for a...      F   \n",
       "2      A 40-year-old man comes to the physician becau...      E   \n",
       "3      A 4-year-old boy is brought to the emergency d...      E   \n",
       "4      A 9-year-old girl is brought to the emergency ...      C   \n",
       "...                                                  ...    ...   \n",
       "14364  A 9-year-old boy is brought to the clinic by h...      A   \n",
       "14365  A 3-month-old infant is brought to the E.R. by...      B   \n",
       "14366  A 36-year-old woman with no significant medica...      A   \n",
       "14367  A 68-year-old male is brought to his primary c...      A   \n",
       "14368  A 7-year-old girl presents for a follow-up vis...      D   \n",
       "\n",
       "                                                 options meta_info  \\\n",
       "0      {'A': 'Nerve conduction study', 'B': 'Surgical...     step2   \n",
       "1      {'A': 'Cirrhosis', 'B': 'Acute lymphoblastic l...     step2   \n",
       "2      {'A': 'Asphyxia', 'B': 'Achlorhydria', 'C': 'T...     step2   \n",
       "3      {'A': \"Get consent from the patient's brother\"...     step2   \n",
       "4      {'A': 'Oculomotor nerve damage', 'B': 'Retrobu...     step2   \n",
       "...                                                  ...       ...   \n",
       "14364  {'A': 'Defect of the septum secundum', 'B': 'F...     step1   \n",
       "14365  {'A': 'Recent pharyngitis', 'B': 'Recent consu...     step1   \n",
       "14366  {'A': 'Formation of free radicals', 'B': 'Inhi...     step1   \n",
       "14367  {'A': 'Accumulations of beta-pleated sheets', ...     step1   \n",
       "14368  {'A': 'Substantia nigra', 'B': 'Hippocampus', ...     step1   \n",
       "\n",
       "      model_letter                                     model_response  \n",
       "0                E   E Splinting of the arm. The explanation is as...  \n",
       "1                F   F Acute myelogenous leukemia. The patient's s...  \n",
       "2                D   D Megaloblastic anemia. \\n\\nExplanation: \\n\\n...  \n",
       "3                E   E. Perform emergency laparotomy.\\n\\nExplanati...  \n",
       "4                F   F Abducens nerve damage. The patient's sympto...  \n",
       "...            ...                                                ...  \n",
       "14364            B   B Failure of the septum primum to fuse with t...  \n",
       "14365            B   B Recent consumption of honey. The reason for...  \n",
       "14366            D   D Binding to the 50S subunit of the ribosome....  \n",
       "14367            D   D Intracellular rounded hyperphosphorylated t...  \n",
       "14368            D   D Amygdala. The reason for this is that the p...  \n",
       "\n",
       "[14369 rows x 6 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data = pd.read_json(path_or_buf='/home/tgao/Github/llm/data_clean/questions/US/US_qbank.jsonl', lines=True)\n",
    "data = pd.read_csv('/home/tgao/Github/llm/llama_out.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='model_letter'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# data.hist(list(data['model_letter']))\n",
    "data.model_letter.value_counts().plot(kind='bar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\'A\\': \"Get consent from the patient\\'s brother\", \\'B\\': \\'Get consent from the patient\\', \\'C\\': \\'Obtain a court order for surgery\\', \\'D\\': \\'Schedule hospital ethics consult\\', \\'E\\': \\'Perform emergency laparotomy\\', \\'F\\': \\'Delay surgery until parental consent\\'}'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['options'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' E. Perform emergency laparotomy.\\n\\nExplanation: In this scenario, the patient'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['model_response'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_correct = 0\n",
    "n = len(data)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data['model_letter'][i] == data['answer'][i]:\n",
    "        n_correct += 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6379010369545549\n"
     ]
    }
   ],
   "source": [
    "print(n_correct / n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg:  0.1887639089747582\n"
     ]
    }
   ],
   "source": [
    "tot_prob = 0\n",
    "for i in range(len(data)):\n",
    "    tot_prob += 1 / data['options'][i].count(':')\n",
    "\n",
    "print('Avg: ', tot_prob / len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-08 01:45:18 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tgao/.conda/envs/llm/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-08 01:45:21 weight_utils.py:207] Using model weights format ['*.safetensors']\n",
      "INFO 06-08 01:45:24 model_runner.py:146] Loading model weights took 14.9595 GB\n",
      "INFO 06-08 01:45:25 gpu_executor.py:83] # GPU blocks: 27894, # CPU blocks: 2048\n",
      "INFO 06-08 01:45:26 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-08 01:45:26 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-08 01:45:30 model_runner.py:924] Graph capturing finished in 4 secs.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import pandas as pd\n",
    "from llm_functions import query_llm\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "llm = LLM(model=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "data = pd.read_csv('/home/tgao/Github/llm/llama_out.csv')\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def get_all_edits(word, n=1):\n",
    "    if n == 1:\n",
    "        return edits1(word)\n",
    "    \n",
    "    last = get_all_edits(word, n-1)\n",
    "\n",
    "    res = set()\n",
    "    for i in last:\n",
    "        cur = edits1(i)\n",
    "        for j in cur:\n",
    "            res.add(j)\n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0)\n",
    "\n",
    "def query_llm(llm, prompts, verbose=False):\n",
    "    outputs = llm.generate(prompts, sampling_params, use_tqdm=verbose)\n",
    "    \n",
    "    return [output.outputs[0].text for output in outputs]\n",
    "\n",
    "def getOptionsString(options):\n",
    "    res = ''\n",
    "    for i in options:\n",
    "        res += i + ' ' + options[i]\n",
    "        res += '\\n'\n",
    "    return res\n",
    "\n",
    "def getLetter(llm_response):\n",
    "    # for i in llm_response.split('\\n'):\n",
    "    #     if(len(i) > 1 and i[0] != '#'):\n",
    "    #         return i[0]\n",
    "    \n",
    "    for i in llm_response:\n",
    "        if i.isupper():\n",
    "            return i\n",
    "    return '*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_subset(f, subset_size=1000):\n",
    "    \"\"\"\n",
    "    Select a random subset of specified size from the list f.\n",
    "\n",
    "    Parameters:\n",
    "    f (list): The list to select from.\n",
    "    subset_size (int): The size of the subset to select.\n",
    "\n",
    "    Returns:\n",
    "    list: A random subset of the original list.\n",
    "    \"\"\"\n",
    "    if subset_size > len(f):\n",
    "        raise ValueError(\"Subset size is larger than the size of the input list\")\n",
    "    \n",
    "    return random.sample(f, subset_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_json(path_or_buf='/home/tgao/Github/llm/data_clean/questions/US/US_qbank.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-08 02:02:47 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='meta-llama/Meta-Llama-3-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Meta-Llama-3-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=meta-llama/Meta-Llama-3-8B-Instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 112.00 MiB. GPU ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Meta-Llama-3-8B-Instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[1;32m      4\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/vllm/entrypoints/llm.py:144\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable_log_stats\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    124\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    125\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    126\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    143\u001b[0m )\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/vllm/engine/llm_engine.py:359\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context)\u001b[0m\n\u001b[1;32m    356\u001b[0m     executor_class \u001b[38;5;241m=\u001b[39m GPUExecutor\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 359\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/vllm/engine/llm_engine.py:222\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, vision_language_config, speculative_config, decoding_config, executor_class, log_stats, usage_context)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config_fields \u001b[38;5;241m=\u001b[39m _load_generation_config_dict(\n\u001b[1;32m    220\u001b[0m     model_config)\n\u001b[0;32m--> 222\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvision_language_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_language_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeculative_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeculative_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39membedding_mode:\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/vllm/executor/executor_base.py:41\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, vision_language_config, speculative_config)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvision_language_config \u001b[38;5;241m=\u001b[39m vision_language_config\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeculative_config \u001b[38;5;241m=\u001b[39m speculative_config\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/vllm/executor/gpu_executor.py:24\u001b[0m, in \u001b[0;36mGPUExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_worker()\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker\u001b[38;5;241m.\u001b[39minit_device()\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/vllm/worker/worker.py:121\u001b[0m, in \u001b[0;36mWorker.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 121\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/vllm/worker/model_runner.py:134\u001b[0m, in \u001b[0;36mModelRunner.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m CudaMemoryProfiler() \u001b[38;5;28;01mas\u001b[39;00m m:\n\u001b[0;32m--> 134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m            \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    139\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvision_language_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvision_language_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    140\u001b[0m \u001b[43m            \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m            \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mconsumed_memory\n\u001b[1;32m    146\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model weights took \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    147\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m))\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/vllm/model_executor/model_loader/__init__.py:21\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(model_config, load_config, device_config, parallel_config, scheduler_config, lora_config, vision_language_config, cache_config)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_model\u001b[39m(\u001b[38;5;241m*\u001b[39m, model_config: ModelConfig, load_config: LoadConfig,\n\u001b[1;32m     15\u001b[0m               device_config: DeviceConfig, parallel_config: ParallelConfig,\n\u001b[1;32m     16\u001b[0m               scheduler_config: SchedulerConfig,\n\u001b[1;32m     17\u001b[0m               lora_config: Optional[LoRAConfig],\n\u001b[1;32m     18\u001b[0m               vision_language_config: Optional[VisionLanguageConfig],\n\u001b[1;32m     19\u001b[0m               cache_config: CacheConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[1;32m     20\u001b[0m     loader \u001b[38;5;241m=\u001b[39m get_model_loader(load_config)\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mvision_language_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvision_language_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/vllm/model_executor/model_loader/loader.py:240\u001b[0m, in \u001b[0;36mDefaultModelLoader.load_model\u001b[0;34m(self, model_config, device_config, lora_config, vision_language_config, parallel_config, scheduler_config, cache_config)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_default_torch_dtype(model_config\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(device_config\u001b[38;5;241m.\u001b[39mdevice):\n\u001b[0;32m--> 240\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43m_initialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvision_language_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    243\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_weights(\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_weights_iterator(model_config\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    245\u001b[0m                                    model_config\u001b[38;5;241m.\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m                                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfall_back_to_pt_during_load\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    249\u001b[0m                                        \u001b[38;5;28;01mTrue\u001b[39;00m)), )\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, module \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules():\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/vllm/model_executor/model_loader/loader.py:91\u001b[0m, in \u001b[0;36m_initialize_model\u001b[0;34m(model_config, load_config, lora_config, vision_language_config, cache_config)\u001b[0m\n\u001b[1;32m     88\u001b[0m model_class \u001b[38;5;241m=\u001b[39m get_model_architecture(model_config)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     89\u001b[0m quant_config \u001b[38;5;241m=\u001b[39m _get_quantization_config(model_config, load_config)\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_get_model_initialization_kwargs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvision_language_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/vllm/model_executor/models/llama.py:332\u001b[0m, in \u001b[0;36mLlamaForCausalLM.__init__\u001b[0;34m(self, config, cache_config, quant_config, lora_config)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig \u001b[38;5;241m=\u001b[39m config\n\u001b[0;32m--> 332\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munpadded_vocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lora_config:\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/vllm/model_executor/models/llama.py:262\u001b[0m, in \u001b[0;36mLlamaModel.__init__\u001b[0;34m(self, config, cache_config, quant_config, lora_config)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morg_vocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m VocabParallelEmbedding(\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m    259\u001b[0m     config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    260\u001b[0m     org_num_embeddings\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m    261\u001b[0m )\n\u001b[0;32m--> 262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m    263\u001b[0m     LlamaDecoderLayer(config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    264\u001b[0m                       cache_config\u001b[38;5;241m=\u001b[39mcache_config,\n\u001b[1;32m    265\u001b[0m                       quant_config\u001b[38;5;241m=\u001b[39mquant_config)\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    267\u001b[0m ])\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/vllm/model_executor/models/llama.py:263\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morg_vocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m VocabParallelEmbedding(\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m    259\u001b[0m     config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    260\u001b[0m     org_num_embeddings\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m    261\u001b[0m )\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[0;32m--> 263\u001b[0m     \u001b[43mLlamaDecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    267\u001b[0m ])\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/vllm/model_executor/models/llama.py:200\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.__init__\u001b[0;34m(self, config, cache_config, quant_config)\u001b[0m\n\u001b[1;32m    186\u001b[0m attention_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[1;32m    187\u001b[0m     config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m LlamaAttention(\n\u001b[1;32m    189\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    190\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_attention_heads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    198\u001b[0m     cache_config\u001b[38;5;241m=\u001b[39mcache_config,\n\u001b[1;32m    199\u001b[0m )\n\u001b[0;32m--> 200\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m \u001b[43mLlamaMLP\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_act\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmlp_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    208\u001b[0m                                eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    210\u001b[0m                                         eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/vllm/model_executor/models/llama.py:69\u001b[0m, in \u001b[0;36mLlamaMLP.__init__\u001b[0;34m(self, hidden_size, intermediate_size, hidden_act, quant_config, bias)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_up_proj \u001b[38;5;241m=\u001b[39m MergedColumnParallelLinear(\n\u001b[1;32m     65\u001b[0m     input_size\u001b[38;5;241m=\u001b[39mhidden_size,\n\u001b[1;32m     66\u001b[0m     output_sizes\u001b[38;5;241m=\u001b[39m[intermediate_size] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     67\u001b[0m     bias\u001b[38;5;241m=\u001b[39mbias,\n\u001b[1;32m     68\u001b[0m     quant_config\u001b[38;5;241m=\u001b[39mquant_config)\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj \u001b[38;5;241m=\u001b[39m \u001b[43mRowParallelLinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintermediate_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_act \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported activation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhidden_act\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly silu is supported for now.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/vllm/model_executor/layers/linear.py:707\u001b[0m, in \u001b[0;36mRowParallelLinear.__init__\u001b[0;34m(self, input_size, output_size, bias, input_is_parallel, skip_bias_add, params_dtype, reduce_results, quant_config)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size_per_partition \u001b[38;5;241m=\u001b[39m divide(input_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtp_size)\n\u001b[1;32m    706\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 707\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reduce_results \u001b[38;5;129;01mand\u001b[39;00m (bias \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_bias_add):\n\u001b[1;32m    716\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen not reduce the results, adding bias to the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    717\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults can lead to incorrect results\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/vllm/model_executor/layers/linear.py:79\u001b[0m, in \u001b[0;36mUnquantizedLinearMethod.create_weights\u001b[0;34m(self, layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype, **extra_weight_attrs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, layer: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m     75\u001b[0m                    input_size_per_partition: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     76\u001b[0m                    output_partition_sizes: List[\u001b[38;5;28mint\u001b[39m], input_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m     77\u001b[0m                    output_size: \u001b[38;5;28mint\u001b[39m, params_dtype: torch\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m     78\u001b[0m                    \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_weight_attrs):\n\u001b[0;32m---> 79\u001b[0m     weight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m     82\u001b[0m                        requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     83\u001b[0m     set_weight_attrs(weight, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n\u001b[1;32m     84\u001b[0m     layer\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, weight)\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.9/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 112.00 MiB. GPU "
     ]
    }
   ],
   "source": [
    "llm = LLM(model=\"meta-llama/Meta-Llama-3-8B-Instruct\")\n",
    "\n",
    "n_samples = 10\n",
    "idx = 0\n",
    "\n",
    "res = []\n",
    "\n",
    "bads = 0\n",
    "\n",
    "while idx < n_samples:\n",
    "    i = random.choice(range(len(data)))\n",
    "    \n",
    "    if(data['answer'][i] == data['model_letter'][i]):\n",
    "        continue\n",
    "\n",
    "    original_q = data['question'][i]\n",
    "    options_str = getOptionsString(temp['options'][i])\n",
    "    \n",
    "    found = False\n",
    "    \n",
    "    all_qs = []\n",
    "    for j in get_all_edits(original_q, 1):\n",
    "        \n",
    "        query = 'Question -\\n\\n' + j + '\\nChoices -\\n' + options_str\n",
    "        query += '\\nAs an extremely experienced and knowledgable medical professional answering this question accurately, the letter of the correct answer is '\n",
    "        \n",
    "        all_qs.append(query)\n",
    "    \n",
    "    if len(all_qs) > 1000:\n",
    "        all_qs = select_random_subset(all_qs, 1000)\n",
    "    \n",
    "    print(len(all_qs))\n",
    "    \n",
    "    llm_ans = query_llm(llm, all_qs, verbose=True)\n",
    "        \n",
    "    # for i in llm_ans:\n",
    "    #     if(getLetter(i) != data['answer'][i]):\n",
    "    #         print(i, data['answer'][i])\n",
    "    #         print('=' * 30)\n",
    "            \n",
    "    #         res.append((i, data['answer'][i]))\n",
    "            \n",
    "    #         found = True\n",
    "    #         break\n",
    "        \n",
    "    # bads += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
